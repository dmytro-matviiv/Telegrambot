import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict
import logging
from config import NEWS_SOURCES, PUBLISHED_NEWS_FILE, DEFAULT_IMAGE_URL
import re
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.published_news = self.load_published_news()
        self.session = requests.Session()
        self.session.trust_env = False  # –í–∏–º–∏–∫–∞—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ env
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def load_published_news(self) -> set:
        try:
            with open(PUBLISHED_NEWS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('published_news', []))
        except FileNotFoundError:
            return set()

    def save_published_news(self):
        data = {
            'published_news': list(self.published_news),
            'last_updated': datetime.now().isoformat()
        }
        with open(PUBLISHED_NEWS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def translate_text(self, text, source_lang='en', target_lang='uk'):
        """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É —á–µ—Ä–µ–∑ Google Translate"""
        try:
            if not text or len(text.strip()) < 10:
                return text
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ç–µ–∫—Å—Ç –¥—ñ–π—Å–Ω–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é
            if not self.is_english_text(text):
                return text
            
            from deep_translator import GoogleTranslator
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –¥–æ–≤–≥–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (Google Translate –º–∞—î –ª—ñ–º—ñ—Ç)
            max_length = 4000
            if len(text) > max_length:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
                sentences = text.split('. ')
                translated_parts = []
                
                current_part = ""
                for sentence in sentences:
                    if len(current_part + sentence) < max_length:
                        current_part += sentence + ". "
                    else:
                        if current_part:
                            translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                            translated_parts.append(translated_part)
                        current_part = sentence + ". "
                
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É
                if current_part:
                    translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                    translated_parts.append(translated_part)
                
                return " ".join(translated_parts)
            else:
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥—Ä–∞–∑—É
                translated = GoogleTranslator(source='en', target='uk').translate(text)
                logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ —á–µ—Ä–µ–∑ Google Translate: {text[:50]}... ‚Üí {translated[:50]}...")
                return translated
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ Google Translate: {e}")
            # –Ø–∫—â–æ Google Translate –Ω–µ –ø—Ä–∞—Ü—é—î, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç
            return text

    def is_english_text(self, text):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é"""
        if not text:
            return False
        
        # –†–æ–∑—à–∏—Ä–µ–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
        english_indicators = [
            'the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said',
            'news', 'latest', 'breaking', 'report', 'says', 'said', 'ukraine', 'russia', 'war', 'military', 'defense', 'zelensky', 'putin',
            'a', 'an', 'of', 'in', 'on', 'by', 'to', 'is', 'are', 'was', 'were', 'has', 'had', 'would', 'could', 'should',
            'says', 'said', 'reports', 'reported', 'announced', 'announces', 'confirmed', 'confirms',
            'forces', 'troops', 'military', 'defense', 'attack', 'strike', 'bombing', 'shelling',
            'developments', 'situation', 'conflict', 'crisis', 'emergency', 'alert', 'warning'
        ]
        text_lower = text.lower()
        english_count = sum(1 for word in english_indicators if word in text_lower)
        
        # –Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –±—ñ–ª—å—à–µ 2 –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —Å–ª—ñ–≤, –≤–≤–∞–∂–∞—î–º–æ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º
        return english_count >= 2

    def get_news_from_rss(self, source_key: str, source_info: Dict) -> List[Dict]:
        try:
            logger.info(f"üì∞ –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
            rss_urls = [
                source_info.get('rss', ''),
                f"{source_info['website'].rstrip('/')}/rss",
                f"{source_info['website'].rstrip('/')}/feed",
                f"{source_info['website'].rstrip('/')}/rss.xml"
            ]
            feed = None
            for rss_url in rss_urls:
                try:
                    logger.info(f"üîç –ü—Ä–æ–±—É—î–º–æ RSS URL: {rss_url}")
                    response = self.session.get(rss_url, timeout=10, proxies={})
                    logger.info(f"‚Ü™Ô∏è –°—Ç–∞—Ç—É—Å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {response.status_code}")
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        if feed.entries:
                            logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å—ñ–≤ —É RSS")
                            break
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ RSS –∑ {rss_url}: {e}")
                    continue
            if not feed or not feed.entries:
                logger.warning(f"üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
                return []
            # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –¥–∞—Ç–æ—é –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó (—è–∫—â–æ –º–æ–∂–ª–∏–≤–æ)
            entries = sorted(feed.entries, key=lambda e: e.get('published_parsed', None) or e.get('updated_parsed', None) or 0, reverse=True)
            for entry in entries:
                news_id = f"{source_key}_{entry.get('id', entry.get('link', ''))}"
                if news_id in self.published_news:
                    continue
                # --- –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –º–æ–≤–∏ ---
                lang = entry.get('language') or entry.get('lang') or entry.get('dc_language')
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                source_key_lower = source_key.lower()
                is_international_source = any(keyword in source_key_lower for keyword in ['bbc', 'reuters', 'cnn', 'ap', 'guardian', 'nyt', 'washington', 'al_jazeera', 'dw', 'defense', 'war_zone'])
                
                # –î—É–∂–µ –ø—Ä–æ—Å—Ç–∏–π Heuristic: —è–∫—â–æ —è–≤–Ω–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ lang, –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –ª—ñ—Ç–µ—Ä
                def is_ukrainian(text):
                    ukr_letters = set('—ñ—ó—î“ë–Ü–á–Ñ“ê')
                    return any(c in ukr_letters for c in text)
                
                def is_english(text):
                    # –ü—Ä–æ—Å—Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                    common_english_words = ['the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said', 'time', 'people', 'year', 'into', 'just', 'over', 'think', 'also', 'around', 'another', 'come', 'work', 'first', 'well', 'way', 'even', 'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us']
                    text_lower = text.lower()
                    english_word_count = sum(1 for word in common_english_words if word in text_lower)
                    return english_word_count >= 3
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É
                if is_international_source:
                    if lang and not (lang.startswith('uk') or lang.startswith('en')):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é/–∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary) or is_english(title) or is_english(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É/–∞–Ω–≥–ª—ñ–π—Å—å–∫—É (title/summary)")
                        continue
                else:
                    # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –¥–∂–µ—Ä–µ–ª —Ç—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
                    if lang and not lang.startswith('uk'):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É (title/summary)")
                        continue
                # --- –ö—ñ–Ω–µ—Ü—å —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –º–æ–≤–∏ ---
                
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª
                if is_international_source:
                    ukraine_keywords = ['ukraine', 'ukrainian', 'kyiv', 'kiev', 'donetsk', 'luhansk', 'crimea', 'russia', 'russian', 'putin', 'zelensky', 'war', 'conflict', 'invasion', 'military', 'defense', 'weapons', 'sanctions']
                    title_lower = title.lower()
                    summary_lower = summary.lower()
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫—É –∞–±–æ –æ–ø–∏—Å—ñ
                    has_ukraine_keywords = any(keyword in title_lower or keyword in summary_lower for keyword in ukraine_keywords)
                    
                    if not has_ukraine_keywords:
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É: –Ω–µ–º–∞—î –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É/–≤—ñ–π–Ω—É")
                        continue
                
                published = entry.get('published_parsed', None) or entry.get('updated_parsed', None)
                if not published:
                    continue
                published_dt = datetime.fromtimestamp(time.mktime(published))
                if datetime.now() - published_dt > timedelta(hours=5):
                    logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: —Å—Ç–∞—Ä—ñ—à–∞ –∑–∞ 5 –≥–æ–¥–∏–Ω")
                    return []
                full_text = self.get_full_article_text(entry.get('link', ''))
                image_url = self.extract_image_url(entry, entry.get('link', ''))
                video_url = self.extract_video_url(entry, entry.get('link', ''))
                
                # –õ–æ–≥—É—î–º–æ –∑–Ω–∞–π–¥–µ–Ω—ñ –≤—ñ–¥–µ–æ
                if video_url:
                    logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ –Ω–æ–≤–∏–Ω—ñ: {title[:50]}... (URL: {video_url[:50]}...)")
                
                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                if image_url:
                    try:
                        img_resp = self.session.get(image_url, timeout=10, stream=True)
                        from PIL import Image
                        from io import BytesIO
                        img = Image.open(BytesIO(img_resp.content))
                        width, height = img.size
                        if width < 400 or height < 200:
                            logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–æ–∑–º—ñ—Ä {width}x{height}")
                            image_url = DEFAULT_IMAGE_URL
                    except Exception as e:
                        logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —è–∫—ñ—Å—Ç—å ({e})")
                        image_url = DEFAULT_IMAGE_URL
                else:
                    image_url = DEFAULT_IMAGE_URL
                # --- –ö—ñ–Ω–µ—Ü—å –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                
                # --- –ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö –Ω–æ–≤–∏–Ω ---
                is_international_source = any(keyword in source_key.lower() for keyword in ['bbc', 'reuters', 'cnn', 'ap', 'guardian', 'nyt', 'washington', 'al_jazeera', 'dw', 'defense', 'war_zone'])
                
                if is_international_source and self.is_english_text(title):
                    logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É")
                    translated_title = self.translate_text(title)
                    if translated_title != title:
                        title = translated_title
                
                if is_international_source and self.is_english_text(summary):
                    logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ–ø–∏—Å –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É")
                    translated_summary = self.translate_text(summary)
                    if translated_summary != summary:
                        summary = translated_summary
                
                # --- –ö—ñ–Ω–µ—Ü—å –ø–µ—Ä–µ–∫–ª–∞–¥—É ---
                
                news_item = {
                    'id': news_id,
                    'title': title,
                    'description': summary,
                    'full_text': full_text,
                    'link': entry.get('link', ''),
                    'image_url': image_url,
                    'video_url': video_url,
                    'source': source_info['name'],
                    'source_key': source_key,
                    'published': entry.get('published', ''),
                    'timestamp': datetime.now().isoformat()
                }
                return [news_item]  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ª–∏—à–µ –æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É
            return []  # –Ø–∫—â–æ –Ω–µ–º–∞—î –Ω–æ–≤–æ—ó –Ω–æ–≤–∏–Ω–∏
        except Exception as e:
            logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_info['name']}: {e}")
            return []

    def get_full_article_text(self, url: str) -> str:
        try:
            response = self.session.get(url, timeout=15, proxies={})
            if response.status_code != 200:
                return ""

            soup = BeautifulSoup(response.content, 'html.parser')

            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            content_selectors = [
                'article',
                '.content',
                '.article-content',
                '.post-content',
                '.entry-content',
                'main',
                '.main-content'
            ]

            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break

            if not content:
                content = soup.find('body')

            if content:
                text = content.get_text(separator=' ', strip=True)
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                text = ' '.join(lines)
                return text[:2000]

            return ""
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return ""

    def extract_image_url(self, entry, article_url: str) -> str:
        try:
            if hasattr(entry, 'media_content') and entry.media_content:
                return entry.media_content[0]['url']

            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                return entry.media_thumbnail[0]['url']

            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                img = soup.find('img')
                if img and img.get('src'):
                    return img['src']

            if article_url:
                response = self.session.get(article_url, timeout=10, proxies={})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    img_selectors = [
                        'article img',
                        '.article-image img',
                        '.post-image img',
                        '.entry-image img',
                        '.content img',
                        'main img'
                    ]
                    for selector in img_selectors:
                        img = soup.select_one(selector)
                        if img and img.get('src'):
                            src = img['src']
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = 'https://' + article_url.split('/')[2] + src
                            return src
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")
        return ""

    def extract_video_url(self, entry, article_url: str) -> str:
        """–í–∏—Ç—è–≥—É—î URL –≤—ñ–¥–µ–æ –∑ –Ω–æ–≤–∏–Ω–∏"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –≤—ñ–¥–µ–æ
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if media.get('type', '').startswith('video/'):
                        logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ: {media['url'][:50]}...")
                        return media['url']

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ–ø–∏—Å –Ω–∞ –≤—ñ–¥–µ–æ —Ç–µ–≥–∏
            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                video = soup.find('video')
                if video and video.get('src'):
                    logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ —Ç–µ–≥ –≤ –æ–ø–∏—Å—ñ: {video['src'][:50]}...")
                    return video['src']
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ iframe –∑ –≤—ñ–¥–µ–æ (YouTube, Vimeo, —Ç–æ—â–æ)
                iframe = soup.find('iframe')
                if iframe and iframe.get('src'):
                    src = iframe['src']
                    if 'youtube.com' in src or 'youtu.be' in src or 'vimeo.com' in src:
                        logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ iframe –≤—ñ–¥–µ–æ –≤ –æ–ø–∏—Å—ñ: {src[:50]}...")
                        return src

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ–≤–Ω—É —Å—Ç–∞—Ç—Ç—é –Ω–∞ –≤—ñ–¥–µ–æ
            if article_url:
                response = self.session.get(article_url, timeout=10, proxies={})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –®—É–∫–∞—î–º–æ –≤—ñ–¥–µ–æ —Ç–µ–≥–∏
                    video_selectors = [
                        'video',
                        'iframe[src*="youtube"]',
                        'iframe[src*="vimeo"]',
                        'iframe[src*="dailymotion"]',
                        '.video-container iframe',
                        '.video iframe',
                        'article iframe',
                        '.content iframe'
                    ]
                    
                    for selector in video_selectors:
                        video_elem = soup.select_one(selector)
                        if video_elem:
                            src = video_elem.get('src')
                            if src:
                                if src.startswith('//'):
                                    src = 'https:' + src
                                elif src.startswith('/'):
                                    src = 'https://' + article_url.split('/')[2] + src
                                logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ —Å—Ç–∞—Ç—Ç—ñ ({selector}): {src[:50]}...")
                                return src
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –≤—ñ–¥–µ–æ: {e}")
        return ""

    def collect_all_news(self) -> List[Dict]:
        all_news = []
        dead_sources = []  # –°–ø–∏—Å–æ–∫ "–º–µ—Ä—Ç–≤–∏—Ö" –¥–∂–µ—Ä–µ–ª
        for source_key, source_info in NEWS_SOURCES.items():
            try:
                news = self.get_news_from_rss(source_key, source_info)
                if not news:
                    dead_sources.append({
                        'key': source_key,
                        'name': source_info.get('name', source_key),
                        'rss': source_info.get('rss', ''),
                        'website': source_info.get('website', '')
                    })
                all_news.extend(news)
                time.sleep(2)
            except Exception as e:
                logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_key}: {e}")
        if dead_sources:
            logger.warning("\n===== –ú–ï–†–¢–í–Ü –î–ñ–ï–†–ï–õ–ê (–Ω–µ–º–∞—î –Ω–æ–≤–∏–Ω) =====")
            for src in dead_sources:
                logger.warning(f"{src['name']} | RSS: {src['rss']} | –°–∞–π—Ç: {src['website']}")
            logger.warning("===== –ö–Ü–ù–ï–¶–¨ –°–ü–ò–°–ö–£ –ú–ï–†–¢–í–ò–• –î–ñ–ï–†–ï–õ =====\n")
        all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        return all_news

    def mark_as_published(self, news_id: str):
        self.published_news.add(news_id)
        self.save_published_news()

    def cleanup_old_news(self, days: int = 7):
        cutoff_date = datetime.now() - timedelta(days=days)
        old_news = set()

        # –¶–µ –º—ñ—Å—Ü–µ –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –æ—á–∏—â–µ–Ω–Ω—è (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏)

        self.published_news -= old_news
        self.save_published_news()
