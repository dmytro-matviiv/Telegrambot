import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from typing import List, Dict
import logging
from config import NEWS_SOURCES, PUBLISHED_NEWS_FILE, DEFAULT_IMAGE_URL
import re
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_published_date(date_str):
    """–ü–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –∑ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ RSS"""
    if not date_str:
        return None
    
    try:
        # –°–ø—Ä–æ–±—É—î–º–æ RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∏–π –≤ RSS)
        dt = parsedate_to_datetime(date_str)
        # –Ø–∫—â–æ –¥–∞—Ç–∞ timezone-naive, –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ UTC
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, TypeError):
        pass
    
    # –°–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –¥–∞—Ç
    date_formats = [
        "%Y-%m-%dT%H:%M:%S",           # ISO format
        "%Y-%m-%dT%H:%M:%SZ",          # ISO format with Z
        "%Y-%m-%d %H:%M:%S",           # Simple format
        "%a, %d %b %Y %H:%M:%S %z",    # RFC 2822 with timezone
        "%a, %d %b %Y %H:%M:%S",       # RFC 2822 without timezone
        "%a, %d %b %Y %H",             # RFC 2822 incomplete (hour only)
        "%d %b %Y %H:%M:%S",           # Short RFC format
        "%Y-%m-%d",                    # Date only
    ]
    
    for fmt in date_formats:
        try:
            # –û–±—Ä—ñ–∑–∞—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—É
            if fmt == "%Y-%m-%dT%H:%M:%S":
                date_str_clean = date_str[:19]
            elif fmt == "%Y-%m-%d %H:%M:%S":
                date_str_clean = date_str[:19]
            elif fmt == "%Y-%m-%d":
                date_str_clean = date_str[:10]
            else:
                date_str_clean = date_str
            
            dt = datetime.strptime(date_str_clean, fmt)
            # –Ø–∫—â–æ –¥–∞—Ç–∞ timezone-naive, –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ UTC
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError):
            continue
    
    logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É: {date_str}")
    return None

class NewsCollector:
    def __init__(self):
        published_data = self.load_published_news()
        self.published_news = published_data['published_news']
        self.last_source = published_data['last_source']
        self.last_published_time = published_data['last_published_time']
        self.last_category_index = published_data.get('last_category_index', 0)  # –Ü–Ω–¥–µ–∫—Å –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        self.session = requests.Session()
        self.session.trust_env = False  # –í–∏–º–∏–∫–∞—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ env
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def load_published_news(self) -> dict:
        try:
            with open(PUBLISHED_NEWS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {
                    'published_news': set(data.get('published_news', [])),
                    'last_source': data.get('last_source', ''),
                    'last_published_time': data.get('last_published_time', ''),
                    'last_category_index': data.get('last_category_index', 0)
                }
        except FileNotFoundError:
            return {
                'published_news': set(),
                'last_source': '',
                'last_published_time': '',
                'last_category_index': 0
            }

    def save_published_news(self):
        data = {
            'published_news': list(self.published_news),
            'last_source': self.last_source,
            'last_published_time': self.last_published_time,
            'last_category_index': self.last_category_index,
            'last_updated': datetime.now(timezone.utc).isoformat()
        }
        with open(PUBLISHED_NEWS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def translate_text(self, text, source_lang='en', target_lang='uk'):
        """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É —á–µ—Ä–µ–∑ Google Translate"""
        try:
            if not text or len(text.strip()) < 10:
                return text
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ç–µ–∫—Å—Ç –¥—ñ–π—Å–Ω–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é
            if not self.is_english_text(text):
                return text
            
            from deep_translator import GoogleTranslator
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –¥–æ–≤–≥–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (Google Translate –º–∞—î –ª—ñ–º—ñ—Ç)
            max_length = 4000
            if len(text) > max_length:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
                sentences = text.split('. ')
                translated_parts = []
                
                current_part = ""
                for sentence in sentences:
                    if len(current_part + sentence) < max_length:
                        current_part += sentence + ". "
                    else:
                        if current_part:
                            translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                            translated_parts.append(translated_part)
                        current_part = sentence + ". "
                
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É
                if current_part:
                    translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                    translated_parts.append(translated_part)
                
                return " ".join(translated_parts)
            else:
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥—Ä–∞–∑—É
                translated = GoogleTranslator(source='en', target='uk').translate(text)
                logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ —á–µ—Ä–µ–∑ Google Translate: {text[:50]}... ‚Üí {translated[:50]}...")
                return translated
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ Google Translate: {e}")
            # –Ø–∫—â–æ Google Translate –Ω–µ –ø—Ä–∞—Ü—é—î, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç
            return text

    def is_english_text(self, text):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é"""
        if not text:
            return False
        
        # –†–æ–∑—à–∏—Ä–µ–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
        english_indicators = [
            'the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said',
            'news', 'latest', 'breaking', 'report', 'says', 'said', 'ukraine', 'russia', 'war', 'military', 'defense', 'zelensky', 'putin',
            'a', 'an', 'of', 'in', 'on', 'by', 'to', 'is', 'are', 'was', 'were', 'has', 'had', 'would', 'could', 'should',
            'says', 'said', 'reports', 'reported', 'announced', 'announces', 'confirmed', 'confirms',
            'forces', 'troops', 'military', 'defense', 'attack', 'strike', 'bombing', 'shelling',
            'developments', 'situation', 'conflict', 'crisis', 'emergency', 'alert', 'warning'
        ]
        text_lower = text.lower()
        english_count = sum(1 for word in english_indicators if word in text_lower)
        
        # –Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –±—ñ–ª—å—à–µ 2 –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —Å–ª—ñ–≤, –≤–≤–∞–∂–∞—î–º–æ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º
        return english_count >= 2

    def is_ukrainian_content(self, title: str, summary: str) -> bool:
        """–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É"""
        ukr_letters = set('—ñ—ó—î“ë–Ü–á–Ñ“ê')
        text = title + ' ' + summary
        return any(c in ukr_letters for c in text)
    
    def is_good_image_size(self, image_url: str) -> bool:
        """–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–æ—Ç–æ"""
        try:
            response = self.session.get(image_url, timeout=5, stream=True)
            if response.status_code != 200:
                return False
            
            from PIL import Image
            from io import BytesIO
            img = Image.open(BytesIO(response.content))
            width, height = img.size
            return width >= 400 and height >= 200
        except:
            return False
    
    def get_rss_feed(self, rss_url: str):
        """–û—Ç—Ä–∏–º—É—î RSS feed"""
        try:
            response = self.session.get(rss_url, timeout=10)
            if response.status_code == 200:
                return feedparser.parse(response.content)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ RSS –∑ {rss_url}: {e}")
        return None

    def get_news_from_rss(self, source_key: str, source_info: dict) -> List[Dict]:
        """–ó–±–∏—Ä–∞—î –Ω–æ–≤–∏–Ω–∏ –∑ RSS –¥–∂–µ—Ä–µ–ª–∞"""
        try:
            logger.info(f"üì∞ –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']} ({source_info.get('category', 'unknown')})")
            
            # –û—Ç—Ä–∏–º—É—î–º–æ RSS
            feed = self.get_rss_feed(source_info['rss'])
            if not feed or not feed.entries:
                logger.warning(f"üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
                return []
            
            logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å—ñ–≤ —É RSS")
            
            news_list = []
            processed_count = 0
            
            for entry in feed.entries[:10]:  # –û–±–º–µ–∂—É—î–º–æ –¥–æ 10 –Ω–æ–≤–∏–Ω –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                try:
                    # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –Ω–æ–≤–∏–Ω–∏
                    title = entry.get('title', '')
                    summary = entry.get('summary', '')
                    language = source_info.get('language', 'en')
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–æ–≤—É —Ç–∞ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                    if language == 'en':
                        # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ –Ω–æ–≤–∏–Ω–∏
                        if title:
                            title = self.translate_text(title)
                        if summary:
                            summary = self.translate_text(summary)
                        logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ –Ω–æ–≤–∏–Ω—É: {title[:50]}...")
                    elif language == 'uk':
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞
                        if not self.is_ukrainian_content(title, summary):
                            continue
                    
                    # –®–≤–∏–¥–∫–æ —à—É–∫–∞—î–º–æ —Ñ–æ—Ç–æ
                    image_url = self.extract_image_url(entry, entry.get('link', ''))
                    if not image_url:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –±–µ–∑ —Ñ–æ—Ç–æ
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ñ–æ—Ç–æ
                    if not self.is_good_image_size(image_url):
                        continue
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–Ω—É
                    news_item = {
                        'title': title,
                        'description': summary,
                        'link': entry.get('link', ''),
                        'image_url': image_url,
                        'source': source_info['name'],
                        'source_key': source_key,
                        'category': source_info.get('category', 'unknown'),
                        'language': language,
                        'published': entry.get('published', ''),
                        'id': entry.get('id', entry.get('link', ''))
                    }
                    
                    news_list.append(news_item)
                    processed_count += 1
                    
                    # –ó—É–ø–∏–Ω—è—î–º–æ—Å—è –ø—ñ—Å–ª—è –ø–µ—Ä—à–∏—Ö 3 —Ö–æ—Ä–æ—à–∏—Ö –Ω–æ–≤–∏–Ω
                    if processed_count >= 3:
                        break
                        
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –Ω–æ–≤–∏–Ω–∏: {e}")
                    continue
            
            if news_list:
                logger.info(f"‚úÖ {source_info['name']}: –∑–Ω–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤–∏–Ω –∑ —Ñ–æ—Ç–æ")
            else:
                logger.info(f"‚è© {source_info['name']}: –Ω–µ–º–∞—î –Ω–æ–≤–∏–Ω –∑ —Ñ–æ—Ç–æ")
                
            return news_list
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_info['name']}: {e}")
            return []

    def get_full_article_text(self, url: str) -> str:
        try:
            response = self.session.get(url, timeout=15, proxies={})
            if response.status_code != 200:
                return ""

            soup = BeautifulSoup(response.content, 'html.parser')

            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            content_selectors = [
                'article',
                '.content',
                '.article-content',
                '.post-content',
                '.entry-content',
                'main',
                '.main-content'
            ]

            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break

            if not content:
                content = soup.find('body')

            if content:
                text = content.get_text(separator=' ', strip=True)
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                text = ' '.join(lines)
                return text[:2000]

            return ""
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return ""

    def extract_image_url(self, entry, article_url: str) -> str:
        """–í–∏—Ç—è–≥—É—î URL –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –Ω–æ–≤–∏–Ω–∏"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if media.get('type', '').startswith('image/'):
                        logger.info(f"üì∏ –ó–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ: {media['url'][:50]}...")
                        return media['url']

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ–ø–∏—Å –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                
                # –®—É–∫–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–µ–≥–∏
                img = soup.find('img')
                if img and img.get('src'):
                    logger.info(f"üì∏ –ó–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ –æ–ø–∏—Å—ñ: {img['src'][:50]}...")
                    return img['src']

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
            if article_url:
                try:
                    full_text = self.get_full_article_text(article_url)
                    if full_text:
                        soup = BeautifulSoup(full_text, 'html.parser')
                        
                        # –®—É–∫–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ –ø–æ–≤–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ
                        img = soup.find('img')
                        if img and img.get('src'):
                            logger.info(f"üì∏ –ó–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ –ø–æ–≤–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ: {img['src'][:50]}...")
                            return img['src']
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")

            return ""  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ —è–∫—â–æ —Ñ–æ—Ç–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")
            return ""  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ —è–∫—â–æ –ø–æ–º–∏–ª–∫–∞

    def collect_all_news(self) -> List[Dict]:
        """–ó–±–∏—Ä–∞—î –Ω–æ–≤–∏–Ω–∏ –∑ —É—Å—ñ—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ –ø–µ—Ä–µ–º—ñ—à—É—î —ó—Ö"""
        all_news = []
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —ó—Ö –¥–∂–µ—Ä–µ–ª–∞
        categories = {
            'world': ['bbc_world', 'reuters_world', 'cnn_world'],
            'ukraine': ['channel24', 'unian', 'pravda'],
            'inventions': ['techcrunch', 'wired_tech', 'the_verge'],
            'celebrity': ['people', 'eonline'],
            'war': ['defense_news', 'war_zone']
        }
        
        logger.info("üîÑ –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ —É—Å—ñ—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π...")
        
        # –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ —É—Å—ñ—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
        for category_name, category_sources in categories.items():
            logger.info(f"üì∞ –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é: {category_name}")
            
            # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ –¥–∂–µ—Ä–µ–ª–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ
            random.shuffle(category_sources)
            
            # –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ –¥–∂–µ—Ä–µ–ª –ø–æ—Ç–æ—á–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            for source_key in category_sources:
                try:
                    source_info = NEWS_SOURCES.get(source_key)
                    if not source_info:
                        continue
                        
                    news = self.get_news_from_rss(source_key, source_info)
                    if news:
                        all_news.extend(news)
                        logger.info(f"‚úÖ {source_info['name']}: –∑–Ω–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤–∏–Ω")
                    else:
                        logger.info(f"‚è© {source_info['name']}: –Ω–µ–º–∞—î –Ω–æ–≤–∏–Ω")
                        
                except Exception as e:
                    logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –∑ {source_key}: {e}")
                    continue
        
        # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ –Ω–æ–≤–∏–Ω–∏
        if all_news:
            random.shuffle(all_news)
            logger.info(f"üé≤ –ü–µ—Ä–µ–º—ñ—à–∞–Ω–æ {len(all_news)} –Ω–æ–≤–∏–Ω —É –≤–∏–ø–∞–¥–∫–æ–≤–æ–º—É –ø–æ—Ä—è–¥–∫—É")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –≤–∂–µ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω—ñ –Ω–æ–≤–∏–Ω–∏
        new_news = []
        for news in all_news:
            news_id = f"{news['source_key']}_{news['id']}"
            if news_id not in self.published_news:
                new_news.append(news)
        
        if new_news:
            logger.info(f"üì∞ –ó–Ω–∞–π–¥–µ–Ω–æ {len(new_news)} –Ω–æ–≤–∏—Ö –Ω–æ–≤–∏–Ω –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª")
            
            # –ü–æ–∫–∞–∑—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∂–µ—Ä–µ–ª–∞—Ö
            sources_count = {}
            for news in new_news:
                source = news['source']
                sources_count[source] = sources_count.get(source, 0) + 1
            
            logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∂–µ—Ä–µ–ª–∞—Ö:")
            for source, count in sources_count.items():
                logger.info(f"   {source}: {count} –Ω–æ–≤–∏–Ω")
        else:
            logger.info("üì≠ –ù–æ–≤—ñ –Ω–æ–≤–∏–Ω–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            
        return new_news

    def mark_as_published(self, news_id: str, source_key: str = ''):
        self.published_news.add(news_id)
        if source_key:
            self.last_source = source_key
            self.last_published_time = datetime.now(timezone.utc).isoformat()
        self.save_published_news()

    def cleanup_old_news(self, days: int = 7):
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        old_news = set()

        # –¶–µ –º—ñ—Å—Ü–µ –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –æ—á–∏—â–µ–Ω–Ω—è (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏)

        self.published_news -= old_news
        self.save_published_news()
