import asyncio
import aiohttp
import feedparser
import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from typing import List, Dict
import logging
from config import NEWS_SOURCES, PUBLISHED_NEWS_FILE, DEFAULT_IMAGE_URL
import re
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_published_date(date_str):
    """–ü–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –∑ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ RSS"""
    if not date_str:
        return None
    
    try:
        # –°–ø—Ä–æ–±—É—î–º–æ RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∏–π –≤ RSS)
        dt = parsedate_to_datetime(date_str)
        # –Ø–∫—â–æ –¥–∞—Ç–∞ timezone-naive, –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ UTC
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, TypeError):
        pass
    
    # –°–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –¥–∞—Ç
    date_formats = [
        "%Y-%m-%dT%H:%M:%S",           # ISO format
        "%Y-%m-%dT%H:%M:%SZ",          # ISO format with Z
        "%Y-%m-%d %H:%M:%S",           # Simple format
        "%a, %d %b %Y %H:%M:%S %z",    # RFC 2822 with timezone
        "%a, %d %b %Y %H:%M:%S",       # RFC 2822 without timezone
        "%a, %d %b %Y %H",             # RFC 2822 incomplete (hour only)
        "%d %b %Y %H:%M:%S",           # Short RFC format
        "%Y-%m-%d",                    # Date only
    ]
    
    for fmt in date_formats:
        try:
            # –û–±—Ä—ñ–∑–∞—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—É
            if fmt == "%Y-%m-%dT%H:%M:%S":
                date_str_clean = date_str[:19]
            elif fmt == "%Y-%m-%d %H:%M:%S":
                date_str_clean = date_str[:19]
            elif fmt == "%Y-%m-%d":
                date_str_clean = date_str[:10]
            else:
                date_str_clean = date_str
            
            dt = datetime.strptime(date_str_clean, fmt)
            # –Ø–∫—â–æ –¥–∞—Ç–∞ timezone-naive, –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ UTC
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError):
            continue
    
    logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É: {date_str}")
    return None

class NewsCollector:
    def __init__(self):
        published_data = self.load_published_news()
        self.published_news = published_data['published_news']
        self.last_source = published_data['last_source']
        self.last_published_time = published_data['last_published_time']
        self.session = requests.Session()
        self.session.trust_env = False  # –í–∏–º–∏–∫–∞—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ env
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def load_published_news(self) -> dict:
        try:
            with open(PUBLISHED_NEWS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {
                    'published_news': set(data.get('published_news', [])),
                    'last_source': data.get('last_source', ''),
                    'last_published_time': data.get('last_published_time', '')
                }
        except FileNotFoundError:
            return {
                'published_news': set(),
                'last_source': '',
                'last_published_time': ''
            }

    def save_published_news(self):
        data = {
            'published_news': list(self.published_news),
            'last_source': self.last_source,
            'last_published_time': self.last_published_time,
            'last_updated': datetime.now(timezone.utc).isoformat()
        }
        with open(PUBLISHED_NEWS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def translate_text(self, text, source_lang='en', target_lang='uk'):
        """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É —á–µ—Ä–µ–∑ Google Translate"""
        try:
            if not text or len(text.strip()) < 10:
                return text
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ç–µ–∫—Å—Ç –¥—ñ–π—Å–Ω–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é
            if not self.is_english_text(text):
                return text
            
            from deep_translator import GoogleTranslator
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –¥–æ–≤–≥–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (Google Translate –º–∞—î –ª—ñ–º—ñ—Ç)
            max_length = 4000
            if len(text) > max_length:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
                sentences = text.split('. ')
                translated_parts = []
                
                current_part = ""
                for sentence in sentences:
                    if len(current_part + sentence) < max_length:
                        current_part += sentence + ". "
                    else:
                        if current_part:
                            translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                            translated_parts.append(translated_part)
                        current_part = sentence + ". "
                
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É
                if current_part:
                    translated_part = GoogleTranslator(source='en', target='uk').translate(current_part.strip())
                    translated_parts.append(translated_part)
                
                return " ".join(translated_parts)
            else:
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥—Ä–∞–∑—É
                translated = GoogleTranslator(source='en', target='uk').translate(text)
                logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ —á–µ—Ä–µ–∑ Google Translate: {text[:50]}... ‚Üí {translated[:50]}...")
                return translated
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ Google Translate: {e}")
            # –Ø–∫—â–æ Google Translate –Ω–µ –ø—Ä–∞—Ü—é—î, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç
            return text

    def is_english_text(self, text):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é"""
        if not text:
            return False
        
        # –†–æ–∑—à–∏—Ä–µ–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
        english_indicators = [
            'the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said',
            'news', 'latest', 'breaking', 'report', 'says', 'said', 'ukraine', 'russia', 'war', 'military', 'defense', 'zelensky', 'putin',
            'a', 'an', 'of', 'in', 'on', 'by', 'to', 'is', 'are', 'was', 'were', 'has', 'had', 'would', 'could', 'should',
            'says', 'said', 'reports', 'reported', 'announced', 'announces', 'confirmed', 'confirms',
            'forces', 'troops', 'military', 'defense', 'attack', 'strike', 'bombing', 'shelling',
            'developments', 'situation', 'conflict', 'crisis', 'emergency', 'alert', 'warning'
        ]
        text_lower = text.lower()
        english_count = sum(1 for word in english_indicators if word in text_lower)
        
        # –Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –±—ñ–ª—å—à–µ 2 –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —Å–ª—ñ–≤, –≤–≤–∞–∂–∞—î–º–æ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º
        return english_count >= 2

    def get_news_from_rss(self, source_key: str, source_info: Dict) -> List[Dict]:
        try:
            logger.info(f"üì∞ –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
            rss_urls = [
                source_info.get('rss', ''),
                f"{source_info['website'].rstrip('/')}/rss",
                f"{source_info['website'].rstrip('/')}/feed",
                f"{source_info['website'].rstrip('/')}/rss.xml"
            ]
            feed = None
            for rss_url in rss_urls:
                try:
                    logger.info(f"üîç –ü—Ä–æ–±—É—î–º–æ RSS URL: {rss_url}")
                    response = self.session.get(rss_url, timeout=10, proxies={})
                    logger.info(f"‚Ü™Ô∏è –°—Ç–∞—Ç—É—Å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {response.status_code}")
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        if feed.entries:
                            logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å—ñ–≤ —É RSS")
                            break
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ RSS –∑ {rss_url}: {e}")
                    continue
            if not feed or not feed.entries:
                logger.warning(f"üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
                # –î–æ–¥–∞—î–º–æ –∑–∞–≥–ª—É—à–∫—É-–Ω–æ–≤–∏–Ω—É –¥–ª—è –º–µ—Ä—Ç–≤–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞
                stub_news = {
                    'id': f"{source_key}_stub_{int(time.time())}",
                    'title': f"‚ö†Ô∏è –ù–æ–≤–∏–Ω–∏ –∑ –¥–∂–µ—Ä–µ–ª–∞ '{source_info['name']}' —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ",
                    'description': f"–û—Ñ—ñ—Ü—ñ–π–Ω–µ –¥–∂–µ—Ä–µ–ª–æ {source_info['name']} ({source_info.get('website', '')}) –Ω–∞—Ä–∞–∑—ñ –Ω–µ –Ω–∞–¥–∞—î –Ω–æ–≤–∏–Ω–∏. –°–ø—Ä–æ–±—É—î–º–æ –ø—ñ–∑–Ω—ñ—à–µ.",
                    'full_text': "",
                    'link': source_info.get('website', ''),
                    'image_url': DEFAULT_IMAGE_URL,
                    'video_url': "",
                    'source': source_info['name'],
                    'source_key': source_key,
                    'published': datetime.now(timezone.utc).isoformat(),
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
                return [stub_news]
            # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –¥–∞—Ç–æ—é –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó (—è–∫—â–æ –º–æ–∂–ª–∏–≤–æ)
            entries = sorted(feed.entries, key=lambda e: e.get('published_parsed', None) or e.get('updated_parsed', None) or 0, reverse=True)
            for entry in entries:
                news_id = f"{source_key}_{entry.get('id', entry.get('link', ''))}"
                if news_id in self.published_news:
                    continue
                # --- –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –º–æ–≤–∏ ---
                lang = entry.get('language') or entry.get('lang') or entry.get('dc_language')
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                source_key_lower = source_key.lower()
                is_international_source = any(keyword in source_key_lower for keyword in ['bbc', 'reuters', 'cnn', 'ap', 'guardian', 'nyt', 'washington', 'al_jazeera', 'dw', 'defense', 'war_zone'])
                
                # –î—É–∂–µ –ø—Ä–æ—Å—Ç–∏–π Heuristic: —è–∫—â–æ —è–≤–Ω–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ lang, –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –ª—ñ—Ç–µ—Ä
                def is_ukrainian(text):
                    ukr_letters = set('—ñ—ó—î“ë–Ü–á–Ñ“ê')
                    return any(c in ukr_letters for c in text)
                
                def is_english(text):
                    # –ü—Ä–æ—Å—Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                    common_english_words = ['the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said', 'time', 'people', 'year', 'into', 'just', 'over', 'think', 'also', 'around', 'another', 'come', 'work', 'first', 'well', 'way', 'even', 'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us']
                    text_lower = text.lower()
                    english_word_count = sum(1 for word in common_english_words if word in text_lower)
                    return english_word_count >= 3
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É
                if is_international_source:
                    if lang and not (lang.startswith('uk') or lang.startswith('en')):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é/–∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary) or is_english(title) or is_english(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É/–∞–Ω–≥–ª—ñ–π—Å—å–∫—É (title/summary)")
                        continue
                else:
                    # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –¥–∂–µ—Ä–µ–ª —Ç—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
                    if lang and not lang.startswith('uk'):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É (title/summary)")
                        continue
                # --- –ö—ñ–Ω–µ—Ü—å —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –º–æ–≤–∏ ---
                
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª
                if is_international_source:
                    ukraine_keywords = ['ukraine', 'ukrainian', 'kyiv', 'kiev', 'donetsk', 'luhansk', 'crimea', 'russia', 'russian', 'putin', 'zelensky', 'war', 'conflict', 'invasion', 'military', 'defense', 'weapons', 'sanctions']
                    title_lower = title.lower()
                    summary_lower = summary.lower()
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫—É –∞–±–æ –æ–ø–∏—Å—ñ
                    has_ukraine_keywords = any(keyword in title_lower or keyword in summary_lower for keyword in ukraine_keywords)
                    
                    if not has_ukraine_keywords:
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É: –Ω–µ–º–∞—î –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É/–≤—ñ–π–Ω—É")
                        continue
                
                published = entry.get('published_parsed', None) or entry.get('updated_parsed', None)
                if not published:
                    continue
                published_dt = datetime.fromtimestamp(time.mktime(published))
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ timezone-aware –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
                published_dt = published_dt.replace(tzinfo=timezone.utc)
                if datetime.now(timezone.utc) - published_dt > timedelta(hours=5):
                    logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: —Å—Ç–∞—Ä—ñ—à–∞ –∑–∞ 5 –≥–æ–¥–∏–Ω")
                    return []
                full_text = self.get_full_article_text(entry.get('link', ''))
                image_url = self.extract_image_url(entry, entry.get('link', ''))
                video_url = self.extract_video_url(entry, entry.get('link', ''))
                
                # –õ–æ–≥—É—î–º–æ –∑–Ω–∞–π–¥–µ–Ω—ñ –≤—ñ–¥–µ–æ
                if video_url:
                    logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ –Ω–æ–≤–∏–Ω—ñ: {title[:50]}... (URL: {video_url[:50]}...)")
                
                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                if image_url:
                    try:
                        img_resp = self.session.get(image_url, timeout=10, stream=True)
                        from PIL import Image
                        from io import BytesIO
                        img = Image.open(BytesIO(img_resp.content))
                        width, height = img.size
                        if width < 400 or height < 200:
                            logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–æ–∑–º—ñ—Ä {width}x{height}")
                            image_url = DEFAULT_IMAGE_URL
                    except Exception as e:
                        logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —è–∫—ñ—Å—Ç—å ({e})")
                        image_url = DEFAULT_IMAGE_URL
                else:
                    image_url = DEFAULT_IMAGE_URL
                # --- –ö—ñ–Ω–µ—Ü—å –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                
                # --- –ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö –Ω–æ–≤–∏–Ω ---
                is_international_source = any(keyword in source_key.lower() for keyword in ['bbc', 'reuters', 'cnn', 'ap', 'guardian', 'nyt', 'washington', 'al_jazeera', 'dw', 'defense', 'war_zone'])
                
                if is_international_source and self.is_english_text(title):
                    logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É")
                    translated_title = self.translate_text(title)
                    if translated_title != title:
                        title = translated_title
                
                if is_international_source and self.is_english_text(summary):
                    logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –æ–ø–∏—Å –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É")
                    translated_summary = self.translate_text(summary)
                    if translated_summary != summary:
                        summary = translated_summary
                
                # --- –ö—ñ–Ω–µ—Ü—å –ø–µ—Ä–µ–∫–ª–∞–¥—É ---

                # –û–±—Ä—ñ–∑–∞—î–º–æ description —ñ full_text –¥–æ 400 —Å–∏–º–≤–æ–ª—ñ–≤
                max_len = 400
                def trim(text):
                    if not text:
                        return ""
                    if len(text) > max_len:
                        return text[:max_len].rstrip() + "‚Ä¶"
                    return text

                news_item = {
                    'id': news_id,
                    'title': title,
                    'description': trim(summary),
                    'full_text': trim(full_text),
                    'link': entry.get('link', ''),
                    'image_url': image_url,
                    'video_url': video_url,
                    'source': source_info['name'],
                    'source_key': source_key,
                    'published': entry.get('published', ''),
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
                return [news_item]  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ª–∏—à–µ –æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É
            return []  # –Ø–∫—â–æ –Ω–µ–º–∞—î –Ω–æ–≤–æ—ó –Ω–æ–≤–∏–Ω–∏
        except Exception as e:
            logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_info['name']}: {e}")
            return []

    def get_full_article_text(self, url: str) -> str:
        try:
            response = self.session.get(url, timeout=15, proxies={})
            if response.status_code != 200:
                return ""

            soup = BeautifulSoup(response.content, 'html.parser')

            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            content_selectors = [
                'article',
                '.content',
                '.article-content',
                '.post-content',
                '.entry-content',
                'main',
                '.main-content'
            ]

            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break

            if not content:
                content = soup.find('body')

            if content:
                text = content.get_text(separator=' ', strip=True)
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                text = ' '.join(lines)
                return text[:2000]

            return ""
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return ""

    def extract_image_url(self, entry, article_url: str) -> str:
        try:
            if hasattr(entry, 'media_content') and entry.media_content:
                return entry.media_content[0]['url']

            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                return entry.media_thumbnail[0]['url']

            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                img = soup.find('img')
                if img and img.get('src'):
                    return img['src']

            if article_url:
                response = self.session.get(article_url, timeout=10, proxies={})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    img_selectors = [
                        'article img',
                        '.article-image img',
                        '.post-image img',
                        '.entry-image img',
                        '.content img',
                        'main img'
                    ]
                    for selector in img_selectors:
                        img = soup.select_one(selector)
                        if img and img.get('src'):
                            src = img['src']
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = 'https://' + article_url.split('/')[2] + src
                            return src
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")
        return ""

    def extract_video_url(self, entry, article_url: str) -> str:
        """–í–∏—Ç—è–≥—É—î URL –≤—ñ–¥–µ–æ –∑ –Ω–æ–≤–∏–Ω–∏"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –≤—ñ–¥–µ–æ
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if media.get('type', '').startswith('video/'):
                        logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ: {media['url'][:50]}...")
                        return media['url']

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ–ø–∏—Å –Ω–∞ –≤—ñ–¥–µ–æ —Ç–µ–≥–∏
            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                
                # –®—É–∫–∞—î–º–æ –≤—ñ–¥–µ–æ —Ç–µ–≥–∏
                video = soup.find('video')
                if video and video.get('src'):
                    logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ —Ç–µ–≥ –≤ –æ–ø–∏—Å—ñ: {video['src'][:50]}...")
                    return video['src']
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ iframe –∑ –≤—ñ–¥–µ–æ (YouTube, Vimeo, —Ç–æ—â–æ)
                iframes = soup.find_all('iframe')
                for iframe in iframes:
                    src = iframe.get('src')
                    if src:
                        # –†–æ–∑—à–∏—Ä—é—î–º–æ —Å–ø–∏—Å–æ–∫ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
                        video_platforms = [
                            'youtube.com', 'youtu.be', 'vimeo.com', 'facebook.com',
                            'dailymotion.com', 'rutube.ru', 'vk.com', 'ok.ru',
                            'tsn.ua', 'espreso.tv', '24tv.ua', 'hromadske.ua',
                            'suspilne.media', 'pravda.com.ua', 'ukrinform.ua',
                            'nv.ua', 'zn.ua', 'fakty.com.ua', 'obozrevatel.com',
                            'korrespondent.net', 'liga.net', 'ukraina.ru', 'gordonua.com'
                        ]
                        if any(platform in src.lower() for platform in video_platforms):
                            logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ iframe –≤—ñ–¥–µ–æ –≤ –æ–ø–∏—Å—ñ: {src[:50]}...")
                            return src

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ–≤–Ω—É —Å—Ç–∞—Ç—Ç—é –Ω–∞ –≤—ñ–¥–µ–æ
            if article_url:
                response = self.session.get(article_url, timeout=10, proxies={})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –†–æ–∑—à–∏—Ä–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤ –¥–ª—è –≤—ñ–¥–µ–æ
                    video_selectors = [
                        'video',
                        'iframe[src*="youtube"]',
                        'iframe[src*="vimeo"]',
                        'iframe[src*="dailymotion"]',
                        'iframe[src*="facebook"]',
                        'iframe[src*="rutube"]',
                        'iframe[src*="vk"]',
                        'iframe[src*="ok"]',
                        'iframe[src*="tsn"]',
                        'iframe[src*="espreso"]',
                        'iframe[src*="24tv"]',
                        'iframe[src*="hromadske"]',
                        'iframe[src*="suspilne"]',
                        'iframe[src*="pravda"]',
                        'iframe[src*="ukrinform"]',
                        'iframe[src*="nv"]',
                        'iframe[src*="zn"]',
                        'iframe[src*="fakty"]',
                        'iframe[src*="obozrevatel"]',
                        'iframe[src*="korrespondent"]',
                        'iframe[src*="liga"]',
                        'iframe[src*="ukraina"]',
                        'iframe[src*="gordon"]',
                        '.video-container iframe',
                        '.video iframe',
                        'article iframe',
                        '.content iframe',
                        '.article iframe',
                        '.post iframe',
                        '.entry iframe',
                        '[class*="video"] iframe',
                        '[class*="player"] iframe'
                    ]
                    
                    for selector in video_selectors:
                        video_elem = soup.select_one(selector)
                        if video_elem:
                            src = video_elem.get('src')
                            if src:
                                if src.startswith('//'):
                                    src = 'https:' + src
                                elif src.startswith('/'):
                                    src = 'https://' + article_url.split('/')[2] + src
                                logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –≤ —Å—Ç–∞—Ç—Ç—ñ ({selector}): {src[:50]}...")
                                return src
                    
                    # –î–æ–¥–∞—Ç–∫–æ–≤–æ —à—É–∫–∞—î–º–æ –≤—ñ–¥–µ–æ –≤ —Ç–µ–∫—Å—Ç—ñ —Å—Ç–∞—Ç—Ç—ñ
                    article_text = soup.get_text()
                    video_patterns = [
                        r'https?://[^\s<>"]*\.(mp4|avi|mov|mkv|webm)',
                        r'https?://[^\s<>"]*youtube\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*youtu\.be[^\s<>"]*',
                        r'https?://[^\s<>"]*vimeo\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*dailymotion\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*facebook\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*rutube\.ru[^\s<>"]*',
                        r'https?://[^\s<>"]*vk\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*ok\.ru[^\s<>"]*',
                        r'https?://[^\s<>"]*tsn\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*espreso\.tv[^\s<>"]*',
                        r'https?://[^\s<>"]*24tv\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*hromadske\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*suspilne\.media[^\s<>"]*',
                        r'https?://[^\s<>"]*pravda\.com\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*ukrinform\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*nv\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*zn\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*fakty\.com\.ua[^\s<>"]*',
                        r'https?://[^\s<>"]*obozrevatel\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*korrespondent\.net[^\s<>"]*',
                        r'https?://[^\s<>"]*ukraina\.ru[^\s<>"]*',
                        r'https?://[^\s<>"]*gordonua\.com[^\s<>"]*',
                        r'https?://[^\s<>"]*rbc\.ua[^\s<>"]*'
                    ]
                    
                    for pattern in video_patterns:
                        matches = re.findall(pattern, article_text)
                        if matches:
                            video_url = matches[0] if isinstance(matches[0], str) else matches[0][0]
                            logger.info(f"üé• –ó–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–µ–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤ —Ç–µ–∫—Å—Ç—ñ: {video_url[:50]}...")
                            return video_url
                            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –≤—ñ–¥–µ–æ: {e}")
        return ""

    def is_direct_video(self, video_url: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –≤—ñ–¥–µ–æ –º–æ–∂–Ω–∞ –¥–∏–≤–∏—Ç–∏—Å—è –ø—Ä—è–º–æ –≤ Telegram"""
        if not video_url:
            return False
        
        # –ü—Ä—è–º—ñ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª–∏ –º–æ–∂–Ω–∞ –¥–∏–≤–∏—Ç–∏—Å—è –≤ Telegram
        if video_url.endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
            return True
        
        # YouTube, Vimeo, Facebook - —Ü–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è, –Ω–µ –ø—Ä—è–º—ñ –≤—ñ–¥–µ–æ
        if any(platform in video_url.lower() for platform in ['youtube.com', 'youtu.be', 'vimeo.com', 'facebook.com']):
            return False
        
        # –Ü–Ω—à—ñ –≤—ñ–¥–µ–æ –≤–≤–∞–∂–∞—î–º–æ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –ø—Ä—è–º–∏–º–∏
        return True

    def collect_all_news(self) -> List[Dict]:
        all_news = []
        dead_sources = []  # –°–ø–∏—Å–æ–∫ "–º–µ—Ä—Ç–≤–∏—Ö" –¥–∂–µ—Ä–µ–ª
        sources_with_news = []  # –î–∂–µ—Ä–µ–ª–∞, —è–∫—ñ –º–∞—é—Ç—å –Ω–æ–≤–∏–Ω–∏
        sources_without_news = []  # –î–∂–µ—Ä–µ–ª–∞ –±–µ–∑ –Ω–æ–≤–∏–Ω
        
        # –ü–µ—Ä—à–∏–π –ø—Ä–æ—Ö—ñ–¥: –∑–±–∏—Ä–∞—î–º–æ –ø–æ –æ–¥–Ω—ñ–π –Ω–æ–≤–∏–Ω—ñ –∑ –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞
        source_items = list(NEWS_SOURCES.items())
        random.shuffle(source_items)  # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ –ø–æ—Ä—è–¥–æ–∫ –¥–∂–µ—Ä–µ–ª
        
        for source_key, source_info in source_items:
            try:
                news = self.get_news_from_rss(source_key, source_info)
                if news:
                    sources_with_news.append((source_key, source_info, news))
                    logger.info(f"‚úÖ {source_info['name']}: –∑–Ω–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤–∏–Ω")
                else:
                    sources_without_news.append((source_key, source_info))
                    dead_sources.append({
                        'key': source_key,
                        'name': source_info.get('name', source_key),
                        'rss': source_info.get('rss', ''),
                        'website': source_info.get('website', '')
                    })
                time.sleep(2)
            except Exception as e:
                logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_key}: {e}")
                sources_without_news.append((source_key, source_info))
        
        # –†–æ–∑–ø–æ–¥—ñ–ª—è—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑–∞—Ü—ñ—î—é –ø—Ä—è–º–∏—Ö –≤—ñ–¥–µ–æ
        direct_video_news = []  # –ü—Ä—è–º—ñ –≤—ñ–¥–µ–æ (–Ω–∞–πv–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        link_video_news = []    # –í—ñ–¥–µ–æ-–ø–æ—Å–∏–ª–∞–Ω–Ω—è (—Å–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        no_video_news = []      # –ë–µ–∑ –≤—ñ–¥–µ–æ (–Ω–∞–π–Ω–∏–∂—á–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        
        for source_key, source_info, news_list in sources_with_news:
            for news_item in news_list:
                video_url = news_item.get('video_url', '')
                if video_url:
                    if self.is_direct_video(video_url):
                        direct_video_news.append(news_item)
                        logger.info(f"üé¨ –ü–†–Ü–û–†–ò–¢–ï–¢: –ü—Ä—è–º–µ –≤—ñ–¥–µ–æ –≤—ñ–¥ {source_info['name']}: {news_item.get('title', '')[:50]}...")
                    else:
                        link_video_news.append(news_item)
                        logger.info(f"üé• –í—ñ–¥–µ–æ-–ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤—ñ–¥ {source_info['name']}: {news_item.get('title', '')[:50]}...")
                else:
                    no_video_news.append(news_item)
        
        # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ –Ω–æ–≤–∏–Ω–∏ –≤ –∫–æ–∂–Ω—ñ–π –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ –¥–∂–µ—Ä–µ–ª
        random.shuffle(direct_video_news)
        random.shuffle(link_video_news)
        random.shuffle(no_video_news)
        
        # –û–±'—î–¥–Ω—É—î–º–æ: —Å–ø–æ—á–∞—Ç–∫—É –ø—Ä—è–º—ñ –≤—ñ–¥–µ–æ, –ø–æ—Ç—ñ–º –≤—ñ–¥–µ–æ-–ø–æ—Å–∏–ª–∞–Ω–Ω—è, –ø–æ—Ç—ñ–º –±–µ–∑ –≤—ñ–¥–µ–æ
        all_news = direct_video_news + link_video_news + no_video_news

        # --- –î–æ–¥–∞—î–º–æ —Ö–∞–æ—Ç–∏—á–Ω–µ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è –¥–∂–µ—Ä–µ–ª ---
        # –ì—Ä—É–ø—É—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑–∞ source_key
        from collections import defaultdict, deque
        source_groups = defaultdict(deque)
        for news in all_news:
            source_groups[news.get('source_key', '')].append(news)
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ö–∞–æ—Ç–∏—á–Ω—É —á–µ—Ä–≥—É, –ø–æ –æ–¥–Ω—ñ–π –Ω–æ–≤–∏–Ω—ñ –∑ –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞, –ø–æ–∫–∏ —î –Ω–æ–≤–∏–Ω–∏
        shuffled_news = []
        source_keys = list(source_groups.keys())
        random.shuffle(source_keys)
        while any(source_groups.values()):
            for key in source_keys:
                if source_groups[key]:
                    shuffled_news.append(source_groups[key].popleft())
        all_news = shuffled_news
        # --- –ö—ñ–Ω–µ—Ü—å —Ö–∞–æ—Ç–∏—á–Ω–æ–≥–æ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è ---

        # --- –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º (link) ---
        unique_links = set()
        unique_news = []
        for news in all_news:
            link = news.get('link', '')
            if link and link not in unique_links:
                unique_links.add(link)
                unique_news.append(news)
        all_news = unique_news
        # --- –ö—ñ–Ω–µ—Ü—å –≤–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ ---

        # –õ–æ–≥—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–æ—Ä—É –Ω–æ–≤–∏–Ω:")
        logger.info(f"   üé¨ –ü—Ä—è–º—ñ –≤—ñ–¥–µ–æ (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 1): {len(direct_video_news)}")
        logger.info(f"   üé• –í—ñ–¥–µ–æ-–ø–æ—Å–∏–ª–∞–Ω–Ω—è (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 2): {len(link_video_news)}")
        logger.info(f"   üì∞ –ù–æ–≤–∏–Ω–∏ –±–µ–∑ –≤—ñ–¥–µ–æ (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 3): {len(no_video_news)}")
        logger.info(f"   ‚úÖ –ê–∫—Ç–∏–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞: {len(sources_with_news)}")
        logger.info(f"   ‚ùå –ù–µ–∞–∫—Ç–∏–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞: {len(sources_without_news)}")
        
        if dead_sources:
            logger.warning("\n===== –ú–ï–†–¢–í–Ü –î–ñ–ï–†–ï–õ–ê (–Ω–µ–º–∞—î –Ω–æ–≤–∏–Ω) =====")
            for src in dead_sources:
                logger.warning(f"{src['name']} | RSS: {src['rss']} | –°–∞–π—Ç: {src['website']}")
            logger.warning("===== –ö–Ü–ù–ï–¶–¨ –°–ü–ò–°–ö–£ –ú–ï–†–¢–í–ò–• –î–ñ–ï–†–ï–õ =====\n")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–æ–≤–∏–Ω–∏, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–≤—Ç–æ—Ä—ñ–≤ –∑ –æ–¥–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞ –ø—ñ–¥—Ä—è–¥
        filtered_news = []
        if all_news:
            # –î–æ–¥–∞—î–º–æ –ø–µ—Ä—à—É –Ω–æ–≤–∏–Ω—É
            filtered_news.append(all_news[0])
            
            # –î–ª—è —Ä–µ—à—Ç–∏ –Ω–æ–≤–∏–Ω –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–∂–µ—Ä–µ–ª–æ
            for news_item in all_news[1:]:
                current_source = news_item.get('source_key', '')
                
                # –Ø–∫—â–æ —Ü–µ –Ω–µ —Ç–µ —Å–∞–º–µ –¥–∂–µ—Ä–µ–ª–æ, —â–æ –π –æ—Å—Ç–∞–Ω–Ω—î –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–µ, –¥–æ–¥–∞—î–º–æ
                if current_source != self.last_source:
                    filtered_news.append(news_item)
                    break  # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É –∑–∞ —Ä–∞–∑
            
            # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –Ω–æ–≤–∏–Ω—É –∑ —ñ–Ω—à–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞, –±–µ—Ä–µ–º–æ –ø–µ—Ä—à—É –¥–æ—Å—Ç—É–ø–Ω—É
            if len(filtered_news) == 1 and len(all_news) > 1:
                filtered_news.append(all_news[1])
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º –≤—ñ–¥–µ–æ —Ç–∞ —á–∞—Å–æ–º (–∑–∞–ª–∏—à–∞—î–º–æ –¥–ª—è fallback)
        def get_sort_key(news_item):
            video_url = news_item.get('video_url', '')
            
            # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: 0 = –ø—Ä—è–º–µ –≤—ñ–¥–µ–æ, 1 = –≤—ñ–¥–µ–æ-–ø–æ—Å–∏–ª–∞–Ω–Ω—è, 2 = –±–µ–∑ –≤—ñ–¥–µ–æ
            if video_url:
                if self.is_direct_video(video_url):
                    video_priority = 0  # –ù–∞–π–≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
                else:
                    video_priority = 1  # –°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            else:
                video_priority = 2  # –ù–∞–π–Ω–∏–∂—á–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            
            # –ü–∞—Ä—Å–∏–º–æ –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
            published_str = news_item.get('published', '')
            if published_str:
                published_dt = parse_published_date(published_str)
                if published_dt:
                    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ timestamp –±–µ–∑ timezone
                    if published_dt.tzinfo:
                        published_dt = published_dt.replace(tzinfo=None)
                    timestamp = time.mktime(published_dt.timetuple())
                    return (video_priority, -timestamp)
            
            # –Ø–∫—â–æ –¥–∞—Ç–∏ –Ω–µ–º–∞—î, —Å—Ç–∞–≤–∏–º–æ –≤ –∫—ñ–Ω–µ—Ü—å
            return (video_priority, 0)
        
        # filtered_news.sort(key=get_sort_key)  # <-- –±—ñ–ª—å—à–µ –Ω–µ —Å–æ—Ä—Ç—É—î–º–æ —Ç—É—Ç, –±–æ –≤–∂–µ —Ö–∞–æ—Ç–∏—á–Ω–æ
        all_news = filtered_news
        
        return all_news

    def mark_as_published(self, news_id: str, source_key: str = ''):
        self.published_news.add(news_id)
        if source_key:
            self.last_source = source_key
            self.last_published_time = datetime.now(timezone.utc).isoformat()
        self.save_published_news()

    def cleanup_old_news(self, days: int = 7):
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        old_news = set()

        # –¶–µ –º—ñ—Å—Ü–µ –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –æ—á–∏—â–µ–Ω–Ω—è (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏)

        self.published_news -= old_news
        self.save_published_news()
