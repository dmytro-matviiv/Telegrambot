import feedparser
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict
import logging
from config import NEWS_SOURCES, PUBLISHED_NEWS_FILE, DEFAULT_IMAGE_URL

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.published_news = self.load_published_news()
        self.session = requests.Session()
        self.session.trust_env = False  # –í–∏–º–∏–∫–∞—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ env
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def load_published_news(self) -> set:
        try:
            with open(PUBLISHED_NEWS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('published_news', []))
        except FileNotFoundError:
            return set()

    def save_published_news(self):
        data = {
            'published_news': list(self.published_news),
            'last_updated': datetime.now().isoformat()
        }
        with open(PUBLISHED_NEWS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def get_news_from_rss(self, source_key: str, source_info: Dict) -> List[Dict]:
        try:
            logger.info(f"üì∞ –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
            rss_urls = [
                source_info.get('rss', ''),
                f"{source_info['website'].rstrip('/')}/rss",
                f"{source_info['website'].rstrip('/')}/feed",
                f"{source_info['website'].rstrip('/')}/rss.xml"
            ]
            feed = None
            for rss_url in rss_urls:
                try:
                    logger.info(f"üîç –ü—Ä–æ–±—É—î–º–æ RSS URL: {rss_url}")
                    response = self.session.get(rss_url, timeout=10, proxies={})
                    logger.info(f"‚Ü™Ô∏è –°—Ç–∞—Ç—É—Å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {response.status_code}")
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        if feed.entries:
                            logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å—ñ–≤ —É RSS")
                            break
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ RSS –∑ {rss_url}: {e}")
                    continue
            if not feed or not feed.entries:
                logger.warning(f"üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ {source_info['name']}")
                return []
            # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –¥–∞—Ç–æ—é –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó (—è–∫—â–æ –º–æ–∂–ª–∏–≤–æ)
            entries = sorted(feed.entries, key=lambda e: e.get('published_parsed', None) or e.get('updated_parsed', None) or 0, reverse=True)
            for entry in entries:
                news_id = f"{source_key}_{entry.get('id', entry.get('link', ''))}"
                if news_id in self.published_news:
                    continue
                # --- –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –º–æ–≤–∏ ---
                lang = entry.get('language') or entry.get('lang') or entry.get('dc_language')
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                source_key_lower = source_key.lower()
                is_international_source = any(keyword in source_key_lower for keyword in ['bbc', 'reuters', 'cnn', 'ap', 'guardian', 'nyt', 'washington', 'al_jazeera', 'dw', 'defense', 'war_zone'])
                
                # –î—É–∂–µ –ø—Ä–æ—Å—Ç–∏–π Heuristic: —è–∫—â–æ —è–≤–Ω–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ lang, –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –ª—ñ—Ç–µ—Ä
                def is_ukrainian(text):
                    ukr_letters = set('—ñ—ó—î“ë–Ü–á–Ñ“ê')
                    return any(c in ukr_letters for c in text)
                
                def is_english(text):
                    # –ü—Ä–æ—Å—Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                    common_english_words = ['the', 'and', 'for', 'with', 'this', 'that', 'will', 'have', 'been', 'from', 'they', 'said', 'time', 'people', 'year', 'into', 'just', 'over', 'think', 'also', 'around', 'another', 'come', 'work', 'first', 'well', 'way', 'even', 'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us']
                    text_lower = text.lower()
                    english_word_count = sum(1 for word in common_english_words if word in text_lower)
                    return english_word_count >= 3
                
                # –î–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–æ–∑–≤–æ–ª—è—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É
                if is_international_source:
                    if lang and not (lang.startswith('uk') or lang.startswith('en')):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é/–∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary) or is_english(title) or is_english(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É/–∞–Ω–≥–ª—ñ–π—Å—å–∫—É (title/summary)")
                        continue
                else:
                    # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –¥–∂–µ—Ä–µ–ª —Ç—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
                    if lang and not lang.startswith('uk'):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é (lang={lang})")
                        continue
                    if not lang and not (is_ukrainian(title) or is_ukrainian(summary)):
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: –Ω–µ —Å—Ö–æ–∂–∞ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É (title/summary)")
                        continue
                # --- –ö—ñ–Ω–µ—Ü—å —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –º–æ–≤–∏ ---
                
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª
                if is_international_source:
                    ukraine_keywords = ['ukraine', 'ukrainian', 'kyiv', 'kiev', 'donetsk', 'luhansk', 'crimea', 'russia', 'russian', 'putin', 'zelensky', 'war', 'conflict', 'invasion', 'military', 'defense', 'weapons', 'sanctions']
                    title_lower = title.lower()
                    summary_lower = summary.lower()
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫—É –∞–±–æ –æ–ø–∏—Å—ñ
                    has_ukraine_keywords = any(keyword in title_lower or keyword in summary_lower for keyword in ukraine_keywords)
                    
                    if not has_ukraine_keywords:
                        logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É: –Ω–µ–º–∞—î –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É/–≤—ñ–π–Ω—É")
                        continue
                
                published = entry.get('published_parsed', None) or entry.get('updated_parsed', None)
                if not published:
                    continue
                published_dt = datetime.fromtimestamp(time.mktime(published))
                if datetime.now() - published_dt > timedelta(hours=5):
                    logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω—É: —Å—Ç–∞—Ä—ñ—à–∞ –∑–∞ 5 –≥–æ–¥–∏–Ω")
                    return []
                full_text = self.get_full_article_text(entry.get('link', ''))
                image_url = self.extract_image_url(entry, entry.get('link', ''))
                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                if image_url:
                    try:
                        img_resp = self.session.get(image_url, timeout=10, stream=True)
                        from PIL import Image
                        from io import BytesIO
                        img = Image.open(BytesIO(img_resp.content))
                        width, height = img.size
                        if width < 400 or height < 200:
                            logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–æ–∑–º—ñ—Ä {width}x{height}")
                            image_url = DEFAULT_IMAGE_URL
                    except Exception as e:
                        logger.info(f"‚è© –§–æ—Ç–æ –∑–∞–º—ñ–Ω–µ–Ω–æ –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–µ: –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —è–∫—ñ—Å—Ç—å ({e})")
                        image_url = DEFAULT_IMAGE_URL
                else:
                    image_url = DEFAULT_IMAGE_URL
                # --- –ö—ñ–Ω–µ—Ü—å –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —è–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ ---
                news_item = {
                    'id': news_id,
                    'title': title,
                    'description': summary,
                    'full_text': full_text,
                    'link': entry.get('link', ''),
                    'image_url': image_url,
                    'source': source_info['name'],
                    'source_key': source_key,
                    'published': entry.get('published', ''),
                    'timestamp': datetime.now().isoformat()
                }
                return [news_item]  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ª–∏—à–µ –æ–¥–Ω—É –Ω–æ–≤–∏–Ω—É
            return []  # –Ø–∫—â–æ –Ω–µ–º–∞—î –Ω–æ–≤–æ—ó –Ω–æ–≤–∏–Ω–∏
        except Exception as e:
            logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_info['name']}: {e}")
            return []

    def get_full_article_text(self, url: str) -> str:
        try:
            response = self.session.get(url, timeout=15, proxies={})
            if response.status_code != 200:
                return ""

            soup = BeautifulSoup(response.content, 'html.parser')

            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            content_selectors = [
                'article',
                '.content',
                '.article-content',
                '.post-content',
                '.entry-content',
                'main',
                '.main-content'
            ]

            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break

            if not content:
                content = soup.find('body')

            if content:
                text = content.get_text(separator=' ', strip=True)
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                text = ' '.join(lines)
                return text[:2000]

            return ""
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return ""

    def extract_image_url(self, entry, article_url: str) -> str:
        try:
            if hasattr(entry, 'media_content') and entry.media_content:
                return entry.media_content[0]['url']

            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                return entry.media_thumbnail[0]['url']

            if entry.get('summary'):
                soup = BeautifulSoup(entry['summary'], 'html.parser')
                img = soup.find('img')
                if img and img.get('src'):
                    return img['src']

            if article_url:
                response = self.session.get(article_url, timeout=10, proxies={})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    img_selectors = [
                        'article img',
                        '.article-image img',
                        '.post-image img',
                        '.entry-image img',
                        '.content img',
                        'main img'
                    ]
                    for selector in img_selectors:
                        img = soup.select_one(selector)
                        if img and img.get('src'):
                            src = img['src']
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = 'https://' + article_url.split('/')[2] + src
                            return src
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")
        return ""

    def collect_all_news(self) -> List[Dict]:
        all_news = []
        dead_sources = []  # –°–ø–∏—Å–æ–∫ "–º–µ—Ä—Ç–≤–∏—Ö" –¥–∂–µ—Ä–µ–ª
        for source_key, source_info in NEWS_SOURCES.items():
            try:
                news = self.get_news_from_rss(source_key, source_info)
                if not news:
                    dead_sources.append({
                        'key': source_key,
                        'name': source_info.get('name', source_key),
                        'rss': source_info.get('rss', ''),
                        'website': source_info.get('website', '')
                    })
                all_news.extend(news)
                time.sleep(2)
            except Exception as e:
                logger.error(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω –∑ {source_key}: {e}")
        if dead_sources:
            logger.warning("\n===== –ú–ï–†–¢–í–Ü –î–ñ–ï–†–ï–õ–ê (–Ω–µ–º–∞—î –Ω–æ–≤–∏–Ω) =====")
            for src in dead_sources:
                logger.warning(f"{src['name']} | RSS: {src['rss']} | –°–∞–π—Ç: {src['website']}")
            logger.warning("===== –ö–Ü–ù–ï–¶–¨ –°–ü–ò–°–ö–£ –ú–ï–†–¢–í–ò–• –î–ñ–ï–†–ï–õ =====\n")
        all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        return all_news

    def mark_as_published(self, news_id: str):
        self.published_news.add(news_id)
        self.save_published_news()

    def cleanup_old_news(self, days: int = 7):
        cutoff_date = datetime.now() - timedelta(days=days)
        old_news = set()

        # –¶–µ –º—ñ—Å—Ü–µ –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –æ—á–∏—â–µ–Ω–Ω—è (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏)

        self.published_news -= old_news
        self.save_published_news()
